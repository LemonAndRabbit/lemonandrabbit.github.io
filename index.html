<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement"/>
  <meta property="og:title" content="LongMamba"/>
  <meta property="og:description" content="LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement"/>
  <meta property="og:url" content="https://lemonandrabbit.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" /> -->
  <!-- <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->

  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Mamba, SSM, State Space Models, Long Context Understanding, LLMs, Large Language Models"/>
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <style>
    .highlight-text {
      color: #3273dc; /* Bulma's primary blue color */
      font-weight: 600;
    }

    .figure-caption {
      font-style: italic;
      text-align: center;
      /* color: #666; */
      margin-bottom: 1.5rem;
      font-size: 0.95em;
    }

    .green-check {
      color: #22c55e;
      margin-right: 5px;
      font-weight: bold;
    }
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><span class="highlight-text">LongMamba</span>: Enhancing Mamba's Long Context Capabilities via <span class="highlight-text">Training-Free</span> Receptive Field Enlargement</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="." target="_blank">Zhifan Ye</a><sup>1 *</sup>,</span>
                <span class="author-block">
                  <a href="." target="_blank">Kejing Xia</a><sup>1 *</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.yongganfu.com/" target="_blank">Yonggan Fu</a><sup>1,2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://simonxin.com/" target="_blank">Xin Dong</a><sup>2</sup>,
                  </span>
                  <span class=".">
                    <a href="." target="_blank">Jihoon Hong</a><sup>1</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://xiangchi-yuan.github.io/" target="_blank">Xiangchi Yuan</a><sup>1</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://shizhediao.github.io/" target="_blank">Shizhe Diao</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://jankautz.com/" target="_blank">Jan Kautz</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://www.pmolchanov.com/" target="_blank">Pavlo Molchanov</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://eiclab.scs.gatech.edu/pages/team.html#PI" target="_blank">Yingyan (Celine) Lin</a><sup>1,2</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Georgia Institute of Technology, <sup>2</sup>NVIDIA<br>ICLR 2025</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2504.16053" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/GATECH-EIC/LongMamba" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (to be released)</span>
                  </a> -->
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2504.16053" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser figure-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/fig1_overview.png" alt="Overview" class="overview-image">
      <h2 class="subtitle has-text-centered">
        <strong>(a) Key insight</strong>:  SSMs' hidden channels can be categorized into local and global channels, with global channels becoming the bottleneck for long contexts due to limited receptive fields.<br>
        <strong>(b) Our method</strong>: LongMamba enlarges the receptive field of global channels through adaptive token filtering, preserving only the most important tokens in their hidden state memory.
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <strong>TL;DR:</strong> A training-free method for enhancing the long-context capabilities of SSMs.
        <ul style="text-align: left; display: inline-block; margin-top: 10px;">
          <li><span class="green-check">‚úî</span><strong>Effectiveness:</strong> Up to 24.58% average accuracy improvement on RULER;</li>
          <li><span class="green-check">‚úî</span><strong>Compatibility:</strong> Compatible with SSMs and hybrid Transformer-SSM models;</li>
          <li><span class="green-check">‚úî</span><strong>Minimal overhead:</strong> Less than 4% average latency overhead on A100.</li>
        </ul>
      </h2>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            State space models (SSMs) have emerged as an efficient alternative to Transformer models for language modeling, offering linear computational complexity and constant memory usage as context length increases. However, despite their efficiency in handling long contexts, recent studies have shown that SSMs, such as Mamba models, generally underperform compared to Transformers in long-context understanding tasks. To address this significant shortfall and achieve both efficient and accurate long-context understanding, we propose <strong>LongMamba, a training-free technique that significantly enhances the long-context capabilities of Mamba models</strong>. LongMamba builds on our discovery that the hidden channels in Mamba can be categorized into local and global channels based on their receptive field lengths, with global channels primarily responsible for long-context capability. These global channels can become the key bottleneck as the input context lengthens. Specifically, when input lengths largely exceed the training sequence length, global channels exhibit limitations in adaptively extend their receptive fields, leading to Mamba's poor long-context performance. The key idea of LongMamba is to mitigate the hidden state memory decay in these global channels by preventing the accumulation of unimportant tokens in their memory. This is achieved by first identifying critical tokens in the global channels and then applying token filtering to accumulate only those critical tokens. Through extensive benchmarking across synthetic and real-world long-context scenarios, LongMamba sets a new standard for Mamba's long-context performance, significantly extending its operational range without requiring additional training.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">LongMamba: Findings and Analysis</h2>
        <!-- insert an image located at static/images/overview.png-->
        <img src="static/images/fig2_attention.png" alt="Attention" class="attention-image">
        <div class="content has-text-justified mt-4">
            <p class="figure-caption">
              Figure 1: Visualization of the Mamba-130M model's attention map (log scale) of 5 sampled channels under (a) training sequence length (2,000 tokens) and (b) extended sequence length (16,000 tokens).
            </p>

            <p>
              <strong>üîç Finding 1: Hidden state channels in SSMs can be categorized into two classes</strong>, based on their receptive field lengths at training sequence length (Fig. 1(a)):
              <ul>
                <li><strong>Local channels:</strong> Channels exhibit receptive fields that are significantly shorter than the training sequence length, suggesting that these channels function like a convolution layer or a sliding window attention that captures localized information.</li>
                <li><strong>Global channels:</strong> Channels with receptive fields that are comparable to the entire training sequence length, meaning the global channels learn to capture information globally from the entire sequence.</li>
              </ul>
            </p>

            <p>
              <strong>üîç Finding 2: Global channels struggle to keep global attention coverage (global receptive field) at extended context lengths</strong>, as shown in Fig. 1(b), which could limit SSMs' capability for capturing and understanding long-context information.
            </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">LongMamba: A Two-step Pipeline</h2>
        <!-- insert an image located at static/images/overview.png-->
        <img src="static/images/fig1_overview.png" alt="Overview" class="overview-image">
        <div class="content has-text-justified mt-4">
          <p class="figure-caption">
            Figure 2: (a) Challenge of directly applying SSM to a sequence (length denoted as <span class="math">\(S\)</span>) longer than the training sequence length (denoted as <span class="math">\(L\)</span>); (b) the proposed LongMamba framework, where we enlarge the receptive fields of the global channels using the two-step pipeline detailed below.
          </p>
            <br>
            In light of the above findings, where we identify that the limited receptive field of the global channels poses a major bottleneck for long-context understanding (as shown in Fig. 2(a)), the key idea of LongMamba is to mitigate memory decay in the aforementioned global channels by preventing the accumulation of unimportant tokens in their memory. As illustrated in Fig. 2(b), this is achieved through the following two-step pipeline: 
            <ul>
              <li><strong>Step 1: Identifying global channels within a target SSM</strong>. LongMamba first identifies global channels, which enter the following step, while keeping the other channels (i.e., local channels) untouched.</li>
              <li><strong>Step 2: Enlarging the receptive fields of the global channels</strong>. LongMamba selects critical tokens along the sequence and applies token filtering to remove less important tokens from the global channels' memory. As a result, the global channels' receptive fields are enlarged, allowing them to capture global information from the entire sequence.</li>
            </ul>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Evaluation Results</h2>
        <!-- insert an image located at static/images/overview.png-->
        <!-- <img src="static/images/fig3_ruler_acc.png" alt="Attention" class="attention-image"> -->
        <div class="results-table-container">
          <table class="table is-striped is-hoverable is-fullwidth compact-table">
            <thead>
              <tr class="has-background-primary" style="background-color: rgb(70,100,152) !important;">
                <th class="has-text-white">Length</th>
                <th class="has-text-white">Method</th>
                <th class="has-text-white">S1</th>
                <th class="has-text-white">S2</th>
                <th class="has-text-white">S3</th>
                <th class="has-text-white">MK1</th>
                <th class="has-text-white">MV</th>
                <th class="has-text-white">MQ</th>
                <th class="has-text-white">VT</th>
                <th class="has-text-white">CWE</th>
                <th class="has-text-white">FWE</th>
                <th class="has-text-white">QA1</th>
                <th class="has-text-white">QA2</th>
                <th class="has-text-white">Avg.</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="2">16k</td>
                <td>Vanilla</td>
                <td>30.00</td>
                <td>11.00</td>
                <td>7.00</td>
                <td>6.00</td>
                <td>8.00</td>
                <td>0.00</td>
                <td>0.20</td>
                <td>0.10</td>
                <td>13.67</td>
                <td>0.00</td>
                <td>1.00</td>
                <td>7.00</td>
              </tr>
              <tr>
                <td>LongMamba</td>
                <td>79.00</td>
                <td>92.00</td>
                <td>31.00</td>
                <td>23.00</td>
                <td>58.00</td>
                <td>49.25</td>
                <td>0.20</td>
                <td>2.30</td>
                <td>0.67</td>
                <td>1.00</td>
                <td>11.00</td>
                <td>31.58</td>
              </tr>
              <tr><td colspan="14" style="height:10px; background-color:#f5f5f5;"></td></tr>
              <tr>
                <td rowspan="2">24k</td>
                <td>Vanilla</td>
                <td>43.00</td>
                <td>9.00</td>
                <td>0.00</td>
                <td>8.00</td>
                <td>7.50</td>
                <td>0.25</td>
                <td>0.00</td>
                <td>0.00</td>
                <td>1.67</td>
                <td>1.00</td>
                <td>7.00</td>
                <td>7.04</td>
              </tr>
              <tr>
                <td>LongMamba</td>
                <td>56.00</td>
                <td>79.00</td>
                <td>29.00</td>
                <td>25.00</td>
                <td>20.50</td>
                <td>18.00</td>
                <td>7.00</td>
                <td>0.30</td>
                <td>1.67</td>
                <td>2.00</td>
                <td>6.00</td>
                <td>22.22</td>
              </tr>
              <tr><td colspan="14" style="height:10px; background-color:#f5f5f5;"></td></tr>
              <tr>
                <td rowspan="2">32k</td>
                <td>Vanilla</td>
                <td>26.00</td>
                <td>0.00</td>
                <td>0.00</td>
                <td>0.00</td>
                <td>0.00</td>
                <td>0.00</td>
                <td>1.80</td>
                <td>0.10</td>
                <td>1.00</td>
                <td>0.00</td>
                <td>1.00</td>
                <td>2.72</td>
              </tr>
              <tr>
                <td>LongMamba</td>
                <td>34.00</td>
                <td>73.00</td>
                <td>16.00</td>
                <td>17.00</td>
                <td>0.80</td>
                <td>0.50</td>
                <td>4.00</td>
                <td>0.20</td>
                <td>0.67</td>
                <td>2.00</td>
                <td>4.00</td>
                <td>13.83</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="content has-text-justified mt-4">
          <p class="figure-caption">
            Table 1: Per-task accuracy of LongMamba-enhanced and the vanilla Zamba2-1.2B model under different sequence lengths on RULER.
          </p>
        </div>

        <div class="results-table-container">
          <table class="table is-striped is-hoverable is-fullwidth compact-table">
            <thead>
              <tr class="has-background-primary" style="background-color: rgb(70,100,152) !important;">
                <th class="has-text-white">Model</th>
                <th class="has-text-white" colspan="2">Mamba-1.4B</th>
                <th class="has-text-white" colspan="2">Mamba2-1.3B</th>
                <th class="has-text-white" colspan="2">Zamba2-1.2B</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Method</td>
                <td>Vanilla</td>
                <td>LongMamba</td>
                <td>Vanilla</td>
                <td>LongMamba</td>
                <td>Vanilla</td>
                <td>LongMamba</td>
              </tr>
              <tr>
                <td>Average Latency (Overhead)</td>
                <td>6.53</td>
                <td>6.78 (+3.7%)</td>
                <td>6.31</td>
                <td>6.48 (+2.7%)</td>
                <td>2.83</td>
                <td>2.94 (+3.9%)</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="content has-text-justified mt-4">
          <p class="figure-caption">
            Table 2: Average prefilling latency of LongMamba-enhanced and vanilla SSMs/hybrid Transformer-SSM models on an A100 GPU (unit: seconds). The latency is averaged across sequence lengths of 8k, 16k, 32k, and 64k tokens.
          </p>
          <p>
            <strong>üìä Consistent Effectiveness:</strong> As shown in Tab. 1, LongMamba consistently outperforms the vanilla Zamba2-1.2B model across all tasks and sequence lengths, achieving up to 24.58% average accuracy improvement on RULER.
          </p>
          <p>
            <strong>‚ö°Minimal Latency Overhead:</strong> Tab. 2 demonstrates that LongMamba introduces only a minimal latency overhead (< 4% on average) across all three tested models (Mamba-1.4B, Mamba2-1.3B, and Zamba2-1.2B) when processing sequences of varying lengths (8k-64k tokens) on an A100 GPU.
          </p>

          <p>
            For more comprehensive benchmarking results (e.g., on LongBench-E), we invite you to check out <a href="https://arxiv.org/abs/2504.16053" target="_blank">our paper</a> for details.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{
        ye2025longmamba,
        title={LongMamba: Enhancing Mamba's Long-Context Capabilities via Training-Free Receptive Field Enlargement},
        author={Zhifan Ye and Kejing Xia and Yonggan Fu and Xin Dong and Jihoon Hong and Xiangchi Yuan and Shizhe Diao and Jan Kautz and Pavlo Molchanov and Yingyan Celine Lin},
        booktitle={The Thirteenth International Conference on Learning Representations},
        year={2025},
        url={https://openreview.net/forum?id=fMbLszVO1H}
        }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
